{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7a134623-e639-49c5-a8a3-9bf85588f85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.impute import KNNImputer\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "import random\n",
    "import string\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# import statsmodels.api as sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5409a781-3f34-4d79-b30d-60682b6d377c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_identifiers = [\n",
    "        \"RowNumber\",\n",
    "        \"ID\",\n",
    "        \"UserID\",\n",
    "        \"UserId\",\n",
    "        \"CustomerId\",\n",
    "        \"User ID\",\n",
    "        \"CustomerID\",\n",
    "        \"ProductID\",\n",
    "        \"EmployeeID\",\n",
    "        \"StudentID\",\n",
    "        \"AccountNumber\",\n",
    "        \"TransactionID\",\n",
    "        \"OrderID\",\n",
    "        \"InvoiceNumber\",\n",
    "        \"SessionID\",\n",
    "        \"ActivityID\",\n",
    "        \"VisitID\",\n",
    "        \"UUID\",\n",
    "        \"UUID(UniversallyUniqueIdentifier)\",\n",
    "        \"SocialSecurityNumber(SSN)\",\n",
    "        \"DriversLicenseNumber\",\n",
    "        \"BusinessLicenseNumber\",\n",
    "        \"LoyaltyProgramID\",\n",
    "        \"MembershipID\",\n",
    "        \"RegistrationNumber\",\n",
    "        \"RollNumber\",\n",
    "        \"User ID\",\n",
    "        \"Customer ID\",\n",
    "        \"Product ID\",\n",
    "        \"Employee ID\",\n",
    "        \"Student ID\",\n",
    "        \"Account Number\",\n",
    "        \"Transaction ID\",\n",
    "        \"Order ID\",\n",
    "        \"Invoice Number\",\n",
    "        \"Session ID\",\n",
    "        \"Activity ID\",\n",
    "        \"Visit ID\",\n",
    "        \"UUID (Universally Unique Identifier)\",\n",
    "        \"Social Security Number (SSN)\",\n",
    "        \"Driver's License Number\",\n",
    "        \"Business License Number\",\n",
    "        \"Loyalty Program ID\",\n",
    "        \"Membership ID\",\n",
    "        \"Registration Number\",\n",
    "        \"Roll Number\",\n",
    "    ]\n",
    "usefullColumns = [\n",
    "    'Revenue',\n",
    "    'Customer Satisfaction Score',\n",
    "    'Net Promoter Score',\n",
    "    'Sales Volume',\n",
    "    'Churn Rate',\n",
    "    'Patient Age',\n",
    "    'Blood Pressure',\n",
    "    'Cholesterol Level',\n",
    "    'Body Mass Index',\n",
    "    'Blood Sugar Level',\n",
    "    'Temperature',\n",
    "    'Humidity',\n",
    "    'Air Quality Index',\n",
    "    'Precipitation Levels',\n",
    "    'Wind Speed',\n",
    "    'Income Level',\n",
    "    'Education Level',\n",
    "    'Employment Status',\n",
    "    'Age',\n",
    "    'Marital Status',\n",
    "    'Product Ratings',\n",
    "    'User Engagement Metrics',\n",
    "    'Inventory Levels',\n",
    "    'Market Share',\n",
    "    'Cost per Acquisition',\n",
    "    'Geographic Location',\n",
    "    'Date',\n",
    "    'Product Category',\n",
    "    'Customer Segment'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d37332d7-afed-41e1-929d-09b8493ee9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class NullCheck:\n",
    "    def __init__(self):\n",
    "        self.droppingColumns = []  # Initialize the list of columns to drop\n",
    "    \n",
    "    # Null Check: Check for missing values and display summary\n",
    "    def nullCheck(self, df):\n",
    "        \"\"\"\n",
    "        Checks for missing values in the DataFrame and displays the total number of missing values,\n",
    "        along with the missing values per column.\n",
    "\n",
    "        Parameters:\n",
    "        df (pandas.DataFrame): The DataFrame to check for missing values.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        totalNullsCombineColumns = df.isnull().sum().sum()\n",
    "        if totalNullsCombineColumns != 0:\n",
    "            print(\"Total Nulls: \", totalNullsCombineColumns)\n",
    "            print(\"Null Count in Each Column:\")\n",
    "            print(df.isnull().sum())\n",
    "        else:\n",
    "            print(\"No Null Values found\")\n",
    "\n",
    "    def isCategorical(self, df, column):\n",
    "        \"\"\"\n",
    "        Determines whether a column in the DataFrame is categorical.\n",
    "\n",
    "        Parameters:\n",
    "        df (pandas.DataFrame): The DataFrame to check the column in.\n",
    "        column (str): The name of the column to check.\n",
    "\n",
    "        Returns:\n",
    "        bool: True if the column is categorical, otherwise False.\n",
    "        \"\"\"\n",
    "        return df[column].dtype == 'object'\n",
    "\n",
    "    # Automate removing missing values with the threshold for missing data\n",
    "    def automateRemovingNullValues(self, df, threshold=0.1):\n",
    "        \"\"\"\n",
    "        Automates the process of removing missing values with a threshold for missing data.\n",
    "\n",
    "        Parameters:\n",
    "        df (pandas.DataFrame): The DataFrame to process.\n",
    "        threshold (float): The threshold of missing values in a column above which it is handled.\n",
    "\n",
    "        Returns:\n",
    "        pandas.DataFrame: The DataFrame after missing values have been handled.\n",
    "        \"\"\"\n",
    "        for column in df.columns:\n",
    "            # Skip columns that are too large\n",
    "            if len(df[column]) > 1000000:\n",
    "                continue  # Need a better approach to handle very large columns\n",
    "\n",
    "            # Handle categorical columns (replace NaNs with Mode)\n",
    "            if self.isCategorical(df, column):\n",
    "                df = self.replaceWithMode(df, column)\n",
    "            else:\n",
    "                # Check if the column is numeric (not categorical)\n",
    "                if pd.api.types.is_numeric_dtype(df[column]):\n",
    "                    if self.skewCheck(df[column]):\n",
    "                        # If the column is skewed, replace NaNs with Median\n",
    "                        df = self.replaceWithMedian(df, column)\n",
    "                    else:\n",
    "                        # For non-skewed numeric columns, replace NaNs with Mean\n",
    "                        df = self.replaceWithMean(df, column)\n",
    "                else:\n",
    "                    # Handle non-numeric columns\n",
    "                    if df[column].isnull().all():\n",
    "                        df[column].fillna('Unknown', inplace=True)\n",
    "                    else:\n",
    "                        df = self.replaceWithMode(df, column)\n",
    "        return df\n",
    "\n",
    "    # Check for skewness or kurtosis in a numeric column\n",
    "    def skewCheck(self, column, skew_threshold=0.5, kurtosis_threshold=3.0):\n",
    "        \"\"\"\n",
    "        Checks for skewness or kurtosis in a numeric column to determine if it's significantly skewed.\n",
    "\n",
    "        Parameters:\n",
    "        column (pandas.Series): The column to check for skewness and kurtosis.\n",
    "        skew_threshold (float): The threshold above which skewness is considered significant.\n",
    "        kurtosis_threshold (float): The threshold above which kurtosis is considered significant.\n",
    "\n",
    "        Returns:\n",
    "        bool: True if the column is skewed or has heavy tails, otherwise False.\n",
    "        \"\"\"\n",
    "        if not pd.api.types.is_numeric_dtype(column):\n",
    "            return False\n",
    "        skewness = column.skew()\n",
    "        kurt = column.kurtosis()\n",
    "        skewed = abs(skewness) > skew_threshold\n",
    "        heavy_tailed = abs(kurt) > kurtosis_threshold\n",
    "        return skewed or heavy_tailed\n",
    "\n",
    "    # Replace missing values with Mode (for categorical data)\n",
    "    def replaceWithMode(self, df, column):\n",
    "        \"\"\"\n",
    "        Replaces missing values in a categorical column with the mode (most frequent value).\n",
    "\n",
    "        Parameters:\n",
    "        df (pandas.DataFrame): The DataFrame containing the column to be processed.\n",
    "        column (str): The name of the column to process.\n",
    "\n",
    "        Returns:\n",
    "        pandas.DataFrame: The DataFrame with missing values replaced by the mode.\n",
    "        \"\"\"\n",
    "        modeValue = df[column].mode()[0]  # Get the most frequent value\n",
    "        df[column] = df[column].fillna(modeValue)\n",
    "        return df\n",
    "\n",
    "    # Replace missing values with Mean (for numeric data)\n",
    "    def replaceWithMean(self, df, column):\n",
    "        \"\"\"\n",
    "        Replaces missing values in a numeric column with the mean.\n",
    "\n",
    "        Parameters:\n",
    "        df (pandas.DataFrame): The DataFrame containing the column to be processed.\n",
    "        column (str): The name of the column to process.\n",
    "\n",
    "        Returns:\n",
    "        pandas.DataFrame: The DataFrame with missing values replaced by the mean.\n",
    "        \"\"\"\n",
    "        meanValue = df[column].mean()\n",
    "        df[column] = df[column].fillna(meanValue)\n",
    "        return df\n",
    "\n",
    "    # Replace missing values with Median (for skewed numeric data)\n",
    "    def replaceWithMedian(self, df, column):\n",
    "        \"\"\"\n",
    "        Replaces missing values in a skewed numeric column with the median.\n",
    "\n",
    "        Parameters:\n",
    "        df (pandas.DataFrame): The DataFrame containing the column to be processed.\n",
    "        column (str): The name of the column to process.\n",
    "\n",
    "        Returns:\n",
    "        pandas.DataFrame: The DataFrame with missing values replaced by the median.\n",
    "        \"\"\"\n",
    "        medianValue = df[column].median()\n",
    "        df[column] = df[column].fillna(medianValue)\n",
    "        return df\n",
    "\n",
    "    # Replacing simpler data null values using Mode, Median, Mean or KNN\n",
    "    def replacingSimplerDataNullValues(self, df, percentNull=0.4, dropColumns=True):\n",
    "        \"\"\"\n",
    "        Replaces missing values in columns based on the percentage of nulls in each column and data type.\n",
    "\n",
    "        Parameters:\n",
    "        df (pandas.DataFrame): The DataFrame to process.\n",
    "        percentNull (float): The threshold percentage of missing values above which a column will be dropped.\n",
    "        dropColumns (bool): Whether or not to drop columns that have a high percentage of missing values.\n",
    "\n",
    "        Returns:\n",
    "        pandas.DataFrame: The DataFrame with missing values replaced or columns dropped.\n",
    "        \"\"\"\n",
    "        for column in df.columns:\n",
    "            nullCount = df[column].isnull().sum()\n",
    "            nullPercent = nullCount / len(df[column])\n",
    "\n",
    "            if nullPercent > percentNull:\n",
    "                self.droppingColumns.append(column)\n",
    "                continue\n",
    "\n",
    "            if pd.api.types.is_string_dtype(df[column]):\n",
    "                df = self.replaceWithMode(df, column)\n",
    "            elif pd.api.types.is_numeric_dtype(df[column]):\n",
    "                if self.skewCheck(df[column]):\n",
    "                    df = self.replaceWithMedian(df, column)\n",
    "                else:\n",
    "                    df = self.replaceWithMean(df, column)\n",
    "\n",
    "        # Drop columns if specified\n",
    "        if dropColumns:\n",
    "            if self.droppingColumns:\n",
    "                df = df.drop(columns=self.droppingColumns)\n",
    "                print(f\"Dropped columns with high null values: {self.droppingColumns}\")\n",
    "            else:\n",
    "                print(\"No columns were dropped.\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def visualizeMissingData(self, df, heatmap_color='viridis', save_fig=False, fig_prefix='missing_data'):\n",
    "        \"\"\"\n",
    "        Visualizes the missing data in the DataFrame using a heatmap and a bar chart showing missing values per feature.\n",
    "\n",
    "        Parameters:\n",
    "        df (pandas.DataFrame): The DataFrame to visualize missing data in.\n",
    "        heatmap_color (str): The color map to use for the heatmap.\n",
    "        save_fig (bool): Whether to save the figures as images.\n",
    "        fig_prefix (str): The prefix for the saved figure filenames.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        if df.empty:\n",
    "            print(\"DataFrame is empty.\")\n",
    "            return\n",
    "        \n",
    "        # Create a boolean DataFrame for missing values (True = missing)\n",
    "        missing_data = df.isnull()\n",
    "        \n",
    "        # Heatmap of missing values\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.heatmap(missing_data, cmap=heatmap_color, cbar=False, yticklabels=False, xticklabels=df.columns)\n",
    "        plt.title('Missing Data Heatmap', fontsize=16)\n",
    "        plt.xlabel('Features', fontsize=14)\n",
    "        plt.ylabel('Observations', fontsize=14)\n",
    "        \n",
    "        # Save the figure if requested\n",
    "        if save_fig:\n",
    "            plt.savefig(f\"{fig_prefix}_heatmap.png\", bbox_inches='tight')\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "        # Bar chart showing the count of missing values per feature\n",
    "        missing_counts = df.isnull().sum()\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        missing_counts.plot(kind='bar', color='skyblue')\n",
    "        plt.title('Missing Values per Feature', fontsize=16)\n",
    "        plt.ylabel('Number of Missing Values', fontsize=14)\n",
    "        plt.xlabel('Features', fontsize=14)\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # Adding percentages to the bar chart\n",
    "        for index, value in enumerate(missing_counts):\n",
    "            plt.text(index, value, f'{value} ({value/len(df)*100:.1f}%)', ha='center', va='bottom')\n",
    "\n",
    "        # Save the figure if requested\n",
    "        if save_fig:\n",
    "            plt.savefig(f\"{fig_prefix}_missing_counts.png\", bbox_inches='tight')\n",
    "        \n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "73ffb708-bc81-46e4-a028-1776e803c89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isCategorical(df, column_name, cardinality_threshold=0.1):\n",
    "    dtype = df[column_name].dtype\n",
    "    \n",
    "    # Check if the column is already a pandas Categorical type\n",
    "    if isinstance(dtype, pd.CategoricalDtype):\n",
    "        return True\n",
    "    \n",
    "    # Check if the column is of object dtype (i.e., strings or mixed types)\n",
    "    if pd.api.types.is_object_dtype(dtype):\n",
    "        unique_count = len(df[column_name].unique())\n",
    "        total_count = len(df)\n",
    "        \n",
    "        # Heuristic: if unique values are less than the cardinality threshold, consider it categorical\n",
    "        if unique_count < cardinality_threshold * total_count:\n",
    "            return True\n",
    "        \n",
    "    # Check for Boolean columns (often treated as categorical)\n",
    "    if pd.api.types.is_bool_dtype(dtype):\n",
    "        return True\n",
    "    \n",
    "    # If none of the above, return False\n",
    "    return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "10384a03-7b1a-4cc5-ae57-6c9b1d0c5e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutlierDetection:\n",
    "    def __init__(self, df, target_column):\n",
    "        self.df = df.copy()  # Make a copy of the DataFrame to avoid modifying original data\n",
    "        self.target_column = target_column\n",
    "        \n",
    "    def isolation_forest_outliers(self, column, contamination=0.1):\n",
    "        \"\"\"\n",
    "        Detect outliers using the Isolation Forest method.\n",
    "        \"\"\"\n",
    "        data = column.values.reshape(-1, 1)\n",
    "        iso_forest = IsolationForest(contamination=contamination, random_state=42)\n",
    "        outliers = iso_forest.fit_predict(data)\n",
    "        return np.where(outliers == -1)[0].tolist()\n",
    "\n",
    "    \n",
    "    def getIQRRange(self, column, dynamicValue = -1):\n",
    "        \"\"\"\n",
    "        Calculate the IQR (Interquartile Range) and dynamic range for outlier detection.\n",
    "        \"\"\"\n",
    "        sortedData = np.sort(column)\n",
    "        if len(sortedData) <= 1:\n",
    "            return [sortedData[0], sortedData[0]]  # If only 1 or 0 elements, no IQR calculation\n",
    "\n",
    "        Q1 = np.percentile(sortedData, 25)\n",
    "        Q3 = np.percentile(sortedData, 75)\n",
    "        IQR = Q3 - Q1\n",
    "        lowerBound = Q1 - (dynamicValue if dynamicValue != -1 else 1.5) * IQR\n",
    "        upperBound = Q3 + (dynamicValue if dynamicValue != -1 else 1.5) * IQR\n",
    "        return [lowerBound, upperBound]\n",
    "\n",
    "    def iqrOutliers(self, column, valueDynamic=-1):\n",
    "        \"\"\"\n",
    "        Identify outliers in a column based on IQR.\n",
    "        \"\"\"\n",
    "        iqrRange = self.getIQRRange(column, valueDynamic)\n",
    "        outlier_indices = [idx for idx, value in enumerate(column) if value < iqrRange[0] or value > iqrRange[1]]\n",
    "        return outlier_indices\n",
    "\n",
    "    def sdRange(self, column, dynamicValue=-1):\n",
    "        \"\"\"\n",
    "        Calculate the SD (Standard Deviation) range for outlier detection.\n",
    "        \"\"\"\n",
    "        meanValue = column.mean()\n",
    "        stdValue = column.std()\n",
    "        lowerRange = meanValue - (dynamicValue if dynamicValue != -1 else 3) * stdValue\n",
    "        upperRange = meanValue + (dynamicValue if dynamicValue != -1 else 3) * stdValue\n",
    "        return [lowerRange, upperRange]\n",
    "\n",
    "    def sdOutliers(self, column, valueDynamic=-1):\n",
    "        \"\"\"\n",
    "        Identify outliers in a column based on Standard Deviation.\n",
    "        \"\"\"\n",
    "        rangeSd = self.sdRange(column, valueDynamic)\n",
    "        outlierIndices = [idx for idx, value in enumerate(column) if value < rangeSd[0] or value > rangeSd[1]]\n",
    "        return outlierIndices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def skewedDetection(self):\n",
    "        \"\"\"\n",
    "        Detect skewed columns in the dataframe.\n",
    "        Returns a list of skewness values for numeric columns.\n",
    "        \"\"\"\n",
    "        skewedList = []\n",
    "        for column in self.df.columns:\n",
    "            if pd.api.types.is_numeric_dtype(self.df[column]):\n",
    "                if self.df[column].nunique() < 5:  # Ignore very low cardinality columns\n",
    "                    skewedList.append(None)\n",
    "                    continue\n",
    "                skew_value = self.df[column].skew()\n",
    "                if skew_value > 0.5:\n",
    "                    skewedList.append(1)  # Right skewed\n",
    "                elif skew_value < -0.5:\n",
    "                    skewedList.append(-1)  # Left skewed\n",
    "                else:\n",
    "                    skewedList.append(0)  # Not skewed\n",
    "            else:\n",
    "                skewedList.append(None)  # For non-numeric columns\n",
    "        return skewedList\n",
    "\n",
    "    # Returns True if there are outliers, False otherwise\n",
    "\n",
    "    # Existing methods from the previous part would be here...\n",
    "\n",
    "\n",
    "\n",
    "    def showOutliers(self, plot_type=\"boxplot\"):\n",
    "    # Ensure plot_type is a string and is valid\n",
    "        if not isinstance(plot_type, str):\n",
    "            raise ValueError(\"plot_type must be a string\")\n",
    "\n",
    "        if plot_type not in [\"boxplot\", \"scatter\", \"histogram\"]:\n",
    "            raise ValueError(f\"Invalid plot_type: {plot_type}. Supported plot types are ['boxplot'].\")\n",
    "    \n",
    "        for column in self.df.columns:\n",
    "        # Only plot numeric columns\n",
    "            if self.df[column].dtype in ['float64', 'int64']:\n",
    "                sns.set_palette([\"#FFA07A\"])  # Light Salmon (light orange)\n",
    "\n",
    "                sns.set_style(\"whitegrid\")  # Adds a soft white grid background\n",
    "\n",
    "                plt.figure(figsize=(5, 3))\n",
    "\n",
    "                if plot_type == \"boxplot\":\n",
    "                    sns.boxplot(y=self.df[column], color=\"#FF8C00\")  # Skyblue for a calming look\n",
    "                    plt.title(f\"Box Plot of {column}\", fontsize=14, fontweight='bold')\n",
    "                    plt.ylabel(column, fontsize=12)  # Fixed the typo here\n",
    "                elif plot_type == 'scatter':\n",
    "                    plt.scatter(self.df.index, self.df[column], color='#FF8C00', alpha=0.7)  # Light coral for soothing color\n",
    "                    plt.title(f'Scatter Plot of {column}', fontsize=14, fontweight='bold')\n",
    "                    plt.ylabel(column, fontsize=12)\n",
    "                    plt.xlabel('Index', fontsize=12)\n",
    "                elif plot_type == 'histogram':\n",
    "                    sns.histplot(self.df[column], bins=30, kde=True, color='#FF8C00')  # Lightseagreen for a calm histogram color\n",
    "                    plt.title(f'Histogram of {column}', fontsize=14, fontweight='bold')\n",
    "                    plt.xlabel(column, fontsize=12)\n",
    "                    plt.ylabel('Frequency', fontsize=12)\n",
    "        \n",
    "# Adjust font sizes for readability\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "            \n",
    "    def showColumnOutliers(self, column, plot_type='boxplot'):\n",
    "        # Check if the column exists in the DataFrame\n",
    "        if column not in self.df.columns:\n",
    "            print(f\"Column '{column}' does not exist in the DataFrame.\")\n",
    "            return\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        \n",
    "        if plot_type == 'boxplot':\n",
    "            # Create a box plot\n",
    "            sns.boxplot(y=self.df[column])\n",
    "            plt.title(f'Box Plot of {column}')\n",
    "            plt.ylabel(column)\n",
    "\n",
    "        elif plot_type == 'scatter':\n",
    "            # Create a scatter plot\n",
    "            plt.scatter(self.df.index, self.df[column])\n",
    "            plt.title(f'Scatter Plot of {column}')\n",
    "            plt.ylabel(column)\n",
    "            plt.xlabel('Index')\n",
    "\n",
    "        elif plot_type == 'histogram':\n",
    "            # Create a histogram\n",
    "            sns.histplot(self.df[column], bins=30, kde=True)\n",
    "            plt.title(f'Histogram of {column}')\n",
    "            plt.xlabel(column)\n",
    "            plt.ylabel('Frequency')\n",
    "\n",
    "        else:\n",
    "            print(f\"Plot type '{plot_type}' is not supported.\")\n",
    "            return\n",
    "        \n",
    "        plt.show()\n",
    "    def detectOutliersIndex(self, count=False):\n",
    "        all_outliers = {}\n",
    "\n",
    "        # Iterate over all columns\n",
    "        for column in self.df.columns:\n",
    "            if pd.api.types.is_numeric_dtype(self.df[column]):\n",
    "                data = self.df[column].dropna()\n",
    "\n",
    "                # Skip columns with constant values (no variance)\n",
    "                if data.nunique() == 1:\n",
    "                    continue\n",
    "\n",
    "                # Calculate IQR for the column\n",
    "                Q1 = data.quantile(0.25)\n",
    "                Q3 = data.quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "\n",
    "                # Define the bounds for outliers (1.5 * IQR rule)\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "                # Identify the outliers in the column based on index\n",
    "                outlier_indices = data[(data < lower_bound) | (data > upper_bound)].index\n",
    "\n",
    "                # If outliers exist, store either the count or the list of outliers' indices\n",
    "                if not outlier_indices.empty:\n",
    "                    if count:\n",
    "                        all_outliers[column] = len(outlier_indices)  # Store count of outliers\n",
    "                    else:\n",
    "                        all_outliers[column] = outlier_indices.tolist()  # Store list of outlier indices\n",
    "                else:\n",
    "                    # If no outliers detected, add an empty list or a placeholder\n",
    "                    all_outliers[column] = []\n",
    "\n",
    "            elif pd.api.types.is_object_dtype(self.df[column]):\n",
    "                # For categorical data, detect rare categories as outliers\n",
    "                value_counts = self.df[column].value_counts(normalize=True)\n",
    "                rare_categories = value_counts[value_counts < 0.01].index.tolist()  # Less than 1% frequency\n",
    "\n",
    "                # Identify indices where rare categories appear\n",
    "                rare_indices = self.df[column][self.df[column].isin(rare_categories)].index\n",
    "\n",
    "                # If rare categories exist, add their indices as outliers\n",
    "                if not rare_indices.empty:\n",
    "                    if count:\n",
    "                        all_outliers[column] = len(rare_indices)  # Store count of rare category indices\n",
    "                    else:\n",
    "                        all_outliers[column] = rare_indices.tolist()  # Store list of indices for rare categories\n",
    "                else:\n",
    "                    # If no rare categories, add an empty list or a placeholder\n",
    "                    all_outliers[column] = []\n",
    "\n",
    "        return all_outliers\n",
    "\n",
    "    \n",
    "    def detectOutliers(self, count = True):\n",
    "        all_outliers = {}\n",
    "\n",
    "        # Iterate over all columns\n",
    "        for column in self.df.columns:\n",
    "            # Skip non-numeric columns\n",
    "            if pd.api.types.is_numeric_dtype(self.df[column]):\n",
    "                data = self.df[column].dropna()\n",
    "\n",
    "                # Skip columns with constant values (no variance)\n",
    "                if data.nunique() <= 2:\n",
    "                    continue\n",
    "                # Calculate IQR for the column\n",
    "                Q1 = data.quantile(0.25)\n",
    "                Q3 = data.quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "\n",
    "                # Define the bounds for outliers (1.5 * IQR rule)\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "                # Identify the outliers in the column\n",
    "                outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
    "\n",
    "                # If outliers exist, store either the count or the list of outliers\n",
    "                if not outliers.empty:\n",
    "                    if count:\n",
    "                        all_outliers[column] = len(outliers)  # Store count of outliers\n",
    "                    else:\n",
    "                        all_outliers[column] = outliers.tolist()  # Store list of outliers\n",
    "                else:\n",
    "                    # If no outliers detected, add an empty list or a placeholder\n",
    "                    all_outliers[column] = []\n",
    "\n",
    "            elif pd.api.types.is_object_dtype(self.df[column]):\n",
    "                # For categorical data, detect rare categories as outliers\n",
    "                value_counts = self.df[column].value_counts(normalize=True)\n",
    "                rare_categories = value_counts[value_counts < 0.01].index.tolist()  # Less than 1% frequency\n",
    "\n",
    "                # If rare categories exist, add them as outliers\n",
    "                if rare_categories:\n",
    "                    if count:\n",
    "                        all_outliers[column] = len(rare_categories)  # Store count of rare categories\n",
    "                    else:\n",
    "                        all_outliers[column] = rare_categories  # Store the list of rare categories\n",
    "                else:\n",
    "                    # If no rare categories, add an empty list or a placeholder\n",
    "                    all_outliers[column] = []\n",
    "\n",
    "        return all_outliers\n",
    "\n",
    "    def removeOutliers(self):\n",
    "        \n",
    "        all_outliers = self.detectOutliersIndex(count=False)\n",
    "\n",
    "        # Collect all the outlier indices across all columns\n",
    "        outlier_indices_set = set()\n",
    "        for outliers in all_outliers.values():\n",
    "            outlier_indices_set.update(outliers)\n",
    "\n",
    "        # Remove the rows with the outlier indices\n",
    "        self.df = self.df.drop(index=outlier_indices_set)\n",
    "\n",
    "        return self.df\n",
    "\n",
    "\n",
    "        \n",
    "    def detectCategoricalOutliers(self, column_name, threshold_percent=1):\n",
    "        if column_name not in self.df.columns:\n",
    "            raise ValueError(f\"Column '{column_name}' not found in the DataFrame.\")\n",
    "        column = self.df[column_name]\n",
    "        category_counts = column.value_counts()\n",
    "        threshold = len(column) * (threshold_percent / 100)\n",
    "        outliers = category_counts[category_counts < threshold]\n",
    "        return outliers.index.tolist(), outliers.values.tolist()  # Return categories and their counts\n",
    "        \n",
    "    def detectColumnOutliers(self, column, boolean=False):\n",
    "        outliers = {}\n",
    "\n",
    "        # Check if the specified column is numeric\n",
    "        if pd.api.types.is_numeric_dtype(self.df[column]):\n",
    "            columnSkewness = self.df[column].skew()\n",
    "        \n",
    "            # If skewness is high, use the IQR method\n",
    "            if abs(columnSkewness) > 0.5:\n",
    "                outliers[column] = self.iqrOutliers(self.df[column])\n",
    "            else:\n",
    "                # If skewness is low, use the standard deviation method\n",
    "                outliers[column] = self.sdOutliers(self.df[column])\n",
    "\n",
    "            # If `boolean` is True, return True if outliers exist, otherwise False\n",
    "            if boolean:\n",
    "                return len(outliers[column]) > 0  # True if outliers are detected, False otherwise\n",
    "\n",
    "        elif pd.api.types.is_object_dtype(self.df[column]):\n",
    "            # If the column is categorical, use a categorical outlier detection method\n",
    "            outliers[column] = self.detectCategoricalOutliers(column)\n",
    "        \n",
    "            # If `boolean` is True, return True if outliers exist, otherwise False\n",
    "            print(outliers[column])\n",
    "            if boolean:\n",
    "                return len(outliers[column]) > 0  # True if outliers are detected, False otherwise\n",
    "\n",
    "        # If `boolean` is False, return the outliers dictionary for that column\n",
    "            \n",
    "        return outliers\n",
    "\n",
    "    def detectDynamicOutliers(self, boolean=False):\n",
    "        allOutliers = {}\n",
    "\n",
    "        # Check if the target column is numeric\n",
    "        if pd.api.types.is_numeric_dtype(self.df[self.target_column]):\n",
    "            targetSkewness = self.df[self.target_column].skew()\n",
    "            if abs(targetSkewness) > 0.5:\n",
    "                # Use IQR for skewed data\n",
    "                targetOutliers = self.iqrOutliers(self.df[self.target_column])\n",
    "            else:\n",
    "                # Use SD method for data that's not skewed\n",
    "                targetOutliers = self.sdOutliers(self.df[self.target_column])\n",
    "\n",
    "            if len(targetOutliers) != 0:\n",
    "                allOutliers[self.target_column] = targetOutliers\n",
    "        else:\n",
    "            # Categorical data outlier detection\n",
    "            targetOutliers = self.detectCategoricalOutliers(self.target_column)\n",
    "            if len(targetOutliers) != 0:\n",
    "                allOutliers[self.target_column] = targetOutliers\n",
    "\n",
    "        # Iterate through all columns except the target column\n",
    "        for column in self.df.columns:\n",
    "            if column == self.target_column:\n",
    "                continue\n",
    "        \n",
    "            # For numeric columns\n",
    "            if pd.api.types.is_numeric_dtype(self.df[column]):\n",
    "                columnSkewness = self.df[column].skew()\n",
    "                if abs(columnSkewness) > 0.5:\n",
    "                    outliers = self.iqrOutliers(self.df[column])  # Skewed data -> IQR\n",
    "                else:\n",
    "                    outliers = self.sdOutliers(self.df[column])  # Normal data -> SD method\n",
    "            elif pd.api.types.is_object_dtype(self.df[column]):\n",
    "                # For categorical data, use a specific method\n",
    "                outliers = self.detectCategoricalOutliers(column)\n",
    "        \n",
    "            # Only add columns with detected outliers\n",
    "            if len(outliers) != 0:\n",
    "                allOutliers[column] = outliers\n",
    "\n",
    "        # If `boolean` is True, return True if any outliers were detected, otherwise False\n",
    "        if boolean:\n",
    "            return len(allOutliers) > 0  # Return True if there are any outliers, otherwise False\n",
    "\n",
    "        # If `boolean` is False, return the dictionary of all detected outliers\n",
    "        return allOutliers\n",
    "\n",
    "    def handleOutliers(self, series, outliers, method=\"impute\", lower_bound=None, upper_bound=None):\n",
    "        if len(outliers) > 0:\n",
    "            if method == \"remove\":\n",
    "            # Option 1: Remove outliers\n",
    "                series = series[~series.isin(outliers)]\n",
    "\n",
    "            elif method == \"cap\":\n",
    "            # Option 2: Cap outliers to a lower or upper bound (e.g., IQR or SD bounds)\n",
    "                series = series.clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "            elif method == \"impute\":\n",
    "            # Option 3: Impute outliers with a statistic (e.g., mean, median)\n",
    "                median_value = series.median()\n",
    "            # Use .loc to safely modify the original DataFrame or Series\n",
    "                series.loc[series.isin(outliers)] = median_value  # This avoids the SettingWithCopyWarning\n",
    "\n",
    "            else:\n",
    "                print(\"Invalid method specified. Please use 'remove', 'cap', or 'impute'.\")\n",
    "        return series\n",
    "\n",
    "        \n",
    "    def automateOutliers(self, way = \"impute\"):\n",
    "        allOutliers = self.detectOutliers(count = False)\n",
    "        if self.target_column in allOutliers:\n",
    "            if pd.api.types.is_numeric_dtype(self.df[self.target_column]):\n",
    "                targetOutliers = allOutliers[self.target_column]\n",
    "                self.df[self.target_column] = self.handleOutliers(self.df[self.target_column], targetOutliers, way)\n",
    "\n",
    "        for column in self.df.columns:\n",
    "            if pd.api.types.is_numeric_dtype(self.df[column]):\n",
    "                if column in allOutliers:\n",
    "                    columnOutliers = allOutliers[column]\n",
    "                    self.df[column] = self.handleOutliers(self.df[column], columnOutliers, way)\n",
    "            \n",
    "        return self.df\n",
    "      \n",
    "    def check_normality(self):\n",
    "        for column in self.df.columns:\n",
    "            data = self.df[column]\n",
    "            normality_test = NormalityTest(data)\n",
    "            is_normal = normality_test.check_normality()\n",
    "            if is_normal:\n",
    "                continue\n",
    "            else:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8e21fa54-e78e-4811-af8b-121a25174ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixDataTypes:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize with a DataFrame.\"\"\"\n",
    "        pass\n",
    "    def replace_strings_with_nan(self,dataFrame, column):\n",
    "        \n",
    "        if column in dataFrame.columns:\n",
    "            # Apply a lambda function to replace only string values with NaN\n",
    "            dataFrame[column] = dataFrame[column].apply(\n",
    "                lambda x: np.nan if isinstance(x, str) and not x.replace('.', '', 1).isdigit() else x\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Column '{column}' does not exist in the dataframe.\")\n",
    "        return dataFrame\n",
    "    def getCompoundLinearity(self, dataFrame=None):\n",
    "        \"\"\"\n",
    "        This method processes each column of the DataFrame and returns a list of tuples with column names \n",
    "        and their respective linearity details (percentages of numeric, object, and null values).\n",
    "        \"\"\"\n",
    "        # If no DataFrame is passed, use the one from the class instance\n",
    "        if dataFrame is None:\n",
    "            dataFrame = self.dataFrame\n",
    "        \n",
    "        compoundList = []  # List to hold the linearity data for each column\n",
    "        for column in dataFrame.columns:\n",
    "            linearity = self.getLinearity(dataFrame, column, False)\n",
    "            compoundList.append((column, linearity))  # Append the column name and its linearity data\n",
    "        \n",
    "        # Check and print the compound list to confirm it's correctly populated\n",
    "        return compoundList  # Return the list of tuples containing column names and their linearity data\n",
    "    \n",
    "    def getLinearity(self, dataFrame, column, get=True):\n",
    "        \n",
    "        \"\"\"\n",
    "        This method computes the percentage of numeric, object, and null data for a given column.\n",
    "        \"\"\"\n",
    "        isString = dataFrame[column].dtype == \"object\"\n",
    "        linearList = []\n",
    "        tColumn = pd.to_numeric(dataFrame[column], errors=\"coerce\")\n",
    "        \n",
    "            # Count how many numeric values are in the column (non-NaN values)\n",
    "        numCount = tColumn.notna().sum()\n",
    "        objectCount = dataFrame[column].size - numCount\n",
    "        \n",
    "            # Calculate the percentages\n",
    "        numPercentage = numCount / dataFrame[column].size\n",
    "        objectPercentage = objectCount / dataFrame[column].size\n",
    "        nullCount = dataFrame[column].isnull().sum() / dataFrame[column].size\n",
    "        linearList = [numPercentage, objectPercentage, nullCount]\n",
    "        dataLinearList = [\"num\", numPercentage * 100, \"object\", objectPercentage * 100, \"null\", nullCount * 100]\n",
    "        \n",
    "\n",
    "        \n",
    "        return dataLinearList\n",
    "\n",
    "            \n",
    "\n",
    "    \n",
    "    def showDuplicates(self, dataFrame):\n",
    "        linearList = self.getCompoundLinearity(dataFrame)\n",
    "        self.plotLinearityFromList(linearList)\n",
    "        \n",
    "    def plotLinearityFromList(self, linearityData):\n",
    "        \"\"\"\n",
    "        This method takes a list of tuples containing column names and their linearity data,\n",
    "        then generates a stacked bar chart based on the data structure.\n",
    "        \"\"\"\n",
    "        # Check if the linearity data is empty or None\n",
    "        if linearityData is None or len(linearityData) == 0:\n",
    "            print(\"Error: The linearity data is empty or None!\")  # Debug print\n",
    "            return\n",
    "        \n",
    "        # Convert the linearity data into a DataFrame for easy plotting\n",
    "        columns = []\n",
    "        num_percentage = []\n",
    "        object_percentage = []\n",
    "        null_percentage = []\n",
    "\n",
    "        for column, data in linearityData:\n",
    "            columns.append(column)\n",
    "            # Process the linearity data\n",
    "            for i in range(0, len(data), 2):\n",
    "                type_value = data[i]\n",
    "                value = data[i + 1]\n",
    "\n",
    "                if type_value == 'num':\n",
    "                    num_percentage.append(value)\n",
    "                elif type_value == 'object':\n",
    "                    object_percentage.append(value)\n",
    "                elif type_value == 'null':\n",
    "                    null_percentage.append(value)\n",
    "\n",
    "        # Create a DataFrame for plotting the stacked bar chart\n",
    "        linearity_df = pd.DataFrame({\n",
    "            'Column': columns,\n",
    "            'num': num_percentage,\n",
    "            'object': object_percentage,\n",
    "            'null': null_percentage\n",
    "        })\n",
    "\n",
    "        # Plot stacked bar chart\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        linearity_df.set_index('Column').plot(kind='bar', stacked=True, ax=ax, colormap='viridis')\n",
    "\n",
    "        ax.set_title('Data Linearity by Column')\n",
    "        ax.set_ylabel('Percentage (%)')\n",
    "        ax.set_xlabel('Columns')\n",
    "        ax.legend(title='Data Type')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plotMissingValues(self, df, title=\"Missing Data Heatmap\"):\n",
    "        # Plot heatmap for missing data visualization\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.heatmap(df.isnull(), cbar=False, cmap=\"viridis\")\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "       \n",
    "    def replace_numbers_with_unknown(self, dataFrame, column):\n",
    "        if column in dataFrame.columns:\n",
    "        # Use .apply() to transform each value in the column to 'unknown' if it's a number\n",
    "            dataFrame[column] = dataFrame[column].apply(\n",
    "                lambda x: 'unknown' if self.is_number(x) else x\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Column '{column}' does not exist in the dataframe.\")\n",
    "        return dataFrame\n",
    "\n",
    "    def is_number(self, x):\n",
    "        \"\"\"Helper function to check if a value is a number (int, float, or numeric string).\"\"\"\n",
    "    # Handle NaN or None values\n",
    "        if pd.isna(x):\n",
    "            return False\n",
    "    \n",
    "    # Check if the value is a number or can be converted to a number\n",
    "        if isinstance(x, (int, float)): \n",
    "            return True\n",
    "        elif isinstance(x, str):\n",
    "        # Handle cases like '3.14', '1e3', '-5.2', but exclude non-numeric strings\n",
    "            try:\n",
    "                float(x)  # Try to convert the string to a float\n",
    "                return True\n",
    "            except ValueError:\n",
    "                return False  # If it can't be converted to float, it's not a number\n",
    "    \n",
    "        return False\n",
    "\n",
    "\n",
    "    def getSNPercentage(self, dataFrame, column):\n",
    "        isString = dataFrame[column].dtype == \"object\"\n",
    "    \n",
    "        if isString:\n",
    "        # Convert to numeric (invalid parsing will be converted to NaN)\n",
    "            tColumn = pd.to_numeric(dataFrame[column], errors=\"coerce\")\n",
    "        \n",
    "        # Count how many numeric values are in the column (non-NaN values)\n",
    "            numCount = tColumn.notna().sum()\n",
    "            objectCount = dataFrame[column].size - numCount\n",
    "        \n",
    "        # Calculate the percentages\n",
    "            numPercentage = numCount / dataFrame[column].size\n",
    "            objectPercentage = objectCount / dataFrame[column].size\n",
    "        \n",
    "            print(f\"Numeric Percentage: {numPercentage}\")\n",
    "            print(f\"String Percentage: {objectPercentage}\")\n",
    "\n",
    "    \n",
    "    def deepConverting(self, dataFrame, column):\n",
    "        listOfColumns = []\n",
    "    # Check if the column dtype is object (string)\n",
    "        isString = dataFrame[column].dtype == \"object\"\n",
    "    \n",
    "        if isString:\n",
    "        # Convert to numeric (invalid parsing will be converted to NaN)\n",
    "            tColumn = pd.to_numeric(dataFrame[column], errors=\"coerce\")\n",
    "        \n",
    "        # Count how many numeric values are in the column (non-NaN values)\n",
    "            numCount = tColumn.notna().sum()\n",
    "            objectCount = dataFrame[column].size - numCount\n",
    "        \n",
    "        # Calculate the percentages\n",
    "            numPercentage = numCount / dataFrame[column].size\n",
    "            objectPercentage = objectCount / dataFrame[column].size\n",
    "        \n",
    "        # If numeric values are less than 10%, convert all numeric values to \"unknown\"\n",
    "            if objectPercentage < 0.1 and objectPercentage > 0.0:\n",
    "            # Convert numeric values (in string format) to \"unknown\"\n",
    "                dataFrame = self.replace_strings_with_nan(dataFrame, column)\n",
    "        # If string values are less than 10%, convert all string values to NaN\n",
    "            elif numPercentage < 0.1 and numPercentage > 0.0:\n",
    "                print(column)\n",
    "                dataFrame = self.replace_numbers_with_unknown(dataFrame, column)\n",
    "            # Clean up column values if there are mixed values (str and numeric)\n",
    "            else:\n",
    "                listOfColumns.append(column)\n",
    "                \n",
    "        \n",
    "        \n",
    "        return dataFrame\n",
    "\n",
    "    def convert_to_number(self, value):\n",
    "        if isinstance(value, str):\n",
    "        # Try to convert string to float or int\n",
    "            try:\n",
    "                if '.' in value:\n",
    "                    return float(value)  # Convert to float\n",
    "                return int(value)  # Convert to integer\n",
    "            except ValueError:\n",
    "                return value  # Return original string if it cannot be converted\n",
    "        return value  # If it's already a number (int/float), leave it unchanged\n",
    "\n",
    "\n",
    "    def convert_column(self, dataFrame, column):\n",
    "        \"\"\"Convert a specific column to numeric values where applicable.\"\"\"\n",
    "        # Check if the column contains any non-numeric strings\n",
    "        has_non_numeric = dataFrame[column].apply(lambda x: isinstance(x, str) and not (x.isdigit() or self.is_float(x))).any()\n",
    "        if (dataFrame[column].dtype == \"int64\" or dataFrame[column].dtype == \"float64\"):\n",
    "            return dataFrame\n",
    "            \n",
    "        if has_non_numeric:\n",
    "            # If any non-numeric string is found, leave the column as is (don't convert to numeric)\n",
    "            dataFrame = self.deepConverting(dataFrame, column)\n",
    "            \n",
    "        else:\n",
    "            # Convert the column to numeric values (integers or floats)\n",
    "            dataFrame[column] = dataFrame[column].apply(self.convert_to_number)\n",
    "            \n",
    "        return dataFrame\n",
    "\n",
    "    def is_float(self, value):\n",
    "        \"\"\"Helper function to check if a value can be converted to float.\"\"\"\n",
    "        try:\n",
    "            float(value)\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "\n",
    "    def linearDataTypes(self, dataFrame, filterData = True):\n",
    "        \"\"\"Apply column conversions across all columns.\"\"\"\n",
    "        for column in dataFrame.columns:  # Iterate over each column in the DataFrame\n",
    "            dataFrame = self.convert_column(dataFrame, column) \n",
    "        # Apply the conversion to each column\n",
    "        # dataFrame = self.processor.automateRemovingNullValues()\n",
    "        if (filterData):\n",
    "            nullC = NullCheck()\n",
    "            dataFrame = nullC.automateRemovingNullValues(df)\n",
    "            \n",
    "        return dataFrame  # Return the updated DataFrame\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fb35c539-ba27-4d07-aa7a-afd8f1b66964",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Duplicated:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def getDuplicates(self, df):\n",
    "        duplicateValues = {}  # Dictionary to hold duplicated values and their corresponding indexes\n",
    "        # Loop through each column to find duplicate values\n",
    "        for column in df.columns:\n",
    "            duplicateValues[column] = self.getColumnDuplicates(df, column)\n",
    "        return duplicateValues\n",
    "\n",
    "    def getColumnDuplicates(self, df, column):\n",
    "        # Find duplicated values in the column (including the first occurrence)\n",
    "        duplicates = df[df[column].duplicated(keep=False)]  # Keep all duplicates (including the first)\n",
    "        # Create a dictionary to hold the values and their corresponding indexes\n",
    "        value_indexes = {}\n",
    "        \n",
    "        for idx, value in duplicates[column].items():\n",
    "            if value not in value_indexes:\n",
    "                value_indexes[value] = []  # Create a list for each unique duplicated value\n",
    "            value_indexes[value].append(idx)  # Append the index to the list\n",
    "        \n",
    "        return value_indexes\n",
    "   \n",
    "\n",
    "\n",
    "    def replaceDuplicates(self, df):\n",
    "        \"\"\"Replace duplicates in all columns with appropriate values.\"\"\"\n",
    "        for column in df.columns:\n",
    "            df = self.replaceColumnDuplicates(df, column)\n",
    "        return df\n",
    "\n",
    "    def replaceWithMean(self, df, column, duplicateIndices):\n",
    "        \"\"\"Replace duplicates in the column with the mean.\"\"\"\n",
    "        mean_value = df[column].mean()  # Get the mean value\n",
    "    \n",
    "    # Iterate over the duplicate values and their indices\n",
    "        for value, indices in duplicateIndices.items():\n",
    "        # Ensure indices is a list (in case it's not)\n",
    "            if not isinstance(indices, list):\n",
    "                indices = [indices]  # Convert to list if it's not already\n",
    "        \n",
    "            # Skip the first occurrence (keep it), and replace the rest with mean\n",
    "            for idx in indices[1:]:\n",
    "                df.at[idx, column] = mean_value  # Use .at to set the value at a specific index\n",
    "    \n",
    "        return df\n",
    "\n",
    "\n",
    "\n",
    "    def replaceColumnDuplicates(self, df, column):\n",
    "        \"\"\"Identify duplicates in the column and replace them based on their data type.\"\"\"\n",
    "        duplicateList = self.getColumnDuplicates(df, column)  # Get duplicate values and their indices\n",
    "        \n",
    "        # Handle categorical columns (use Mode)\n",
    "        if self.isCategorical(df, column):\n",
    "            return self.replaceWithMode(df, column, duplicateList)\n",
    "        \n",
    "        # Handle numerical columns (use Median or Mean depending on skew)\n",
    "        if pd.api.types.is_numeric_dtype(df[column]):\n",
    "            if self.isSkewed(df[column]):\n",
    "                return self.replaceWithMedian(df, column, duplicateList)\n",
    "            else:\n",
    "                return self.replaceWithMean(df, column, duplicateList)\n",
    "\n",
    "        # Default: For non-numeric, non-categorical (like dates, text), use Mode\n",
    "        return self.replaceWithMode(df, column, duplicateList)\n",
    "\n",
    "    def getColumnDuplicates(self, df, column):\n",
    "        \"\"\"Get duplicate indices for each value in the column.\"\"\"\n",
    "        # Find all duplicates, keeping all occurrences (not just the first)\n",
    "        duplicates = df[df[column].duplicated(keep=False)]\n",
    "        # Group the duplicates by their value, and return the indices in a list\n",
    "        duplicate_indices = duplicates.groupby(column).apply(lambda x: x.index.tolist()).to_dict()\n",
    "        return duplicate_indices\n",
    "\n",
    "\n",
    "    def isCategorical(self, df, column):\n",
    "        \"\"\"Check if the column is categorical.\"\"\"\n",
    "        return df[column].dtype == 'object' or pd.api.types.is_categorical_dtype(df[column])\n",
    "\n",
    "    def isSkewed(self, series):\n",
    "        \"\"\"Check if the data is skewed using skewness metric.\"\"\"\n",
    "        return series.skew() > 1  # A simple rule of thumb for positive skewness\n",
    "\n",
    "    # Additional methods for replacing with Mode and Median, not shown for brevity\n",
    "    def replaceWithMode(self, df, column, duplicateIndices):\n",
    "        \"\"\"Replace duplicates in the column with the mode.\"\"\"\n",
    "        mode_value = df[column].mode()[0]  # Get the most frequent value (mode)\n",
    "    \n",
    "        for value, indices in duplicateIndices.items():\n",
    "            if isinstance(indices, list) and len(indices) > 1:  # Check if indices are a list and have more than one duplicate\n",
    "                # Skip the first occurrence (keep it), and replace the rest with mode\n",
    "                for idx in indices[1:]:\n",
    "                    df.at[idx, column] = mode_value  # Use .at to set the value at a specific index\n",
    "        return df\n",
    "\n",
    "\n",
    "    def replaceWithMedian(self, df, column, duplicateIndices):\n",
    "        \"\"\"Replace duplicates in the column with the median.\"\"\"\n",
    "        median_value = df[column].median()  # Get the median value\n",
    "        # Iterate over the duplicate values and their indices\n",
    "        for value, indices in duplicateIndices.items():\n",
    "            if isinstance(indices, list) and len(indices) > 1:  # Check if indices are a list and have more than one duplicate\n",
    "            \n",
    "            # Skip the first occurrence (keep it), and replace the rest with median\n",
    "                for idx in indices[1:]:\n",
    "                    df.at[idx, column] = median_value  # Use .at to set the value at a specific index\n",
    "        return df\n",
    "    def plotRowDuplicatesBarChart(self, df):\n",
    "        \"\"\"Generate a bar chart showing duplicates vs unique values for each row.\"\"\"\n",
    "        \n",
    "        # Initialize lists to hold the counts for each row\n",
    "        duplicate_counts = []\n",
    "        unique_counts = []\n",
    "\n",
    "        # Iterate over each row to count duplicates and unique values\n",
    "        for idx, row in df.iterrows():\n",
    "            value_counts = row.value_counts()  # Count how many times each value appears in the row\n",
    "            \n",
    "            # Count duplicates (values that appear more than once)\n",
    "            duplicates = sum(value_counts > 1)\n",
    "            unique = len(value_counts) - duplicates  # Unique values are those that appear exactly once\n",
    "            \n",
    "            duplicate_counts.append(duplicates)\n",
    "            unique_counts.append(unique)\n",
    "        \n",
    "        # Create a DataFrame to hold the counts for easy plotting\n",
    "        counts_df = pd.DataFrame({\n",
    "            'Duplicates': duplicate_counts,\n",
    "            'Unique': unique_counts\n",
    "        })\n",
    "\n",
    "        # Plot the bar chart\n",
    "        counts_df.plot(kind='bar', stacked=True, figsize=(10, 6), color=['lightblue', 'lightgreen'])\n",
    "        plt.title('Duplicates vs Unique Values Per Row')\n",
    "        plt.xlabel('Row Index')\n",
    "        plt.ylabel('Count')\n",
    "        plt.xticks(rotation=0)\n",
    "        plt.legend(title='Count Type')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "80e0c603-ee34-4e91-ba89-0ea02bc96c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalityTest:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        \n",
    "    def check_p_value(self, p_value):\n",
    "        \"\"\"\n",
    "        Helper function to return True or False based on the p-value.\n",
    "        \"\"\"\n",
    "        return p_value > 0.05  # Normal if p > 0.05\n",
    "    \n",
    "    def shapiro_wilk_test(self, data):\n",
    "        \"\"\"\n",
    "        Shapiro-Wilk Test for normality\n",
    "        \"\"\"\n",
    "        stat, p_value = stats.shapiro(data)\n",
    "        return self.check_p_value(p_value), p_value\n",
    "\n",
    "    def dagostino_pearson_test(self, data):\n",
    "        \"\"\"\n",
    "        D'Agostino and Pearson's Test for normality\n",
    "        \"\"\"\n",
    "        stat, p_value = stats.normaltest(data)\n",
    "        return self.check_p_value(p_value), p_value\n",
    "\n",
    "    def kolmogorov_smirnov_test(self, data):\n",
    "        \"\"\"\n",
    "        Kolmogorov-Smirnov Test for normality (comparing against normal distribution)\n",
    "        \"\"\"\n",
    "        stat, p_value = stats.kstest(data, 'norm', args=(np.mean(data), np.std(data)))\n",
    "        return self.check_p_value(p_value), p_value\n",
    "    \n",
    "    def jarque_bera_test(self, data):\n",
    "        \"\"\"\n",
    "        Jarque-Bera Test for normality (based on skewness and kurtosis)\n",
    "        \"\"\"\n",
    "        stat, p_value = stats.jarque_bera(data)\n",
    "        return self.check_p_value(p_value), p_value\n",
    "\n",
    "    def check_normality(self):\n",
    "        \"\"\"\n",
    "        Main method to check normality for all numeric columns in the DataFrame.\n",
    "        \"\"\"\n",
    "        normality_results = {}\n",
    "        \n",
    "        # Loop over each numerical column in the DataFrame\n",
    "        for column in self.df.select_dtypes(include=[np.number]).columns:\n",
    "            column_data = self.df[column].dropna()  # Drop missing values\n",
    "\n",
    "            if len(column_data) < 3:  # Skip columns with fewer than 3 data points\n",
    "                normality_results[column] = False\n",
    "                continue\n",
    "            \n",
    "            # Perform the normality tests\n",
    "            tests = [\n",
    "                (\"Shapiro-Wilk Test\", self.shapiro_wilk_test),\n",
    "                (\"D'Agostino and Pearson's Test\", self.dagostino_pearson_test),\n",
    "                (\"Kolmogorov-Smirnov Test\", self.kolmogorov_smirnov_test),\n",
    "                (\"Jarque-Bera Test\", self.jarque_bera_test)\n",
    "            ]\n",
    "            \n",
    "            for test_name, test_func in tests:\n",
    "                is_normal, p_value = test_func(column_data)\n",
    "                if not is_normal:\n",
    "                    normality_results[column] = False\n",
    "                    break\n",
    "            else:\n",
    "                normality_results[column] = True  # If all tests passed, it's normal\n",
    "        \n",
    "        return normality_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "92d08aea-07ae-4503-949f-1443c2121856",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCheck:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def check_high_variation(self, df, column, variance_threshold=0.1):\n",
    "        if column not in df.columns:\n",
    "            print(f\"Column '{column}' not found in the DataFrame.\")\n",
    "            return False\n",
    "        column_data = df[column]\n",
    "        if pd.api.types.is_numeric_dtype(column_data):\n",
    "            std_dev = column_data.std()\n",
    "            if std_dev > variance_threshold:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def checkVariablity(self, df, column, threshold=0.1):\n",
    "        if (column in usefullColumns):\n",
    "            return False\n",
    "    # Check if the column is numeric\n",
    "        column = df[column]\n",
    "        if pd.api.types.is_numeric_dtype(column):\n",
    "            # Check for binary values (0 and 1)\n",
    "            if set(column.unique()).issubset({0, 1}):\n",
    "                return False\n",
    "        \n",
    "            variance = column.var()\n",
    "            uniqueCount = len(column.unique())\n",
    "            rangeValue = column.max() - column.min()\n",
    "        \n",
    "            if variance < threshold and uniqueCount / len(column) < threshold:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def checkHighCardinality(self, column, threshold = 0.1):\n",
    "        uniqueCount = column.nunique()\n",
    "        totalCount = len(column)\n",
    "        ratio = uniqueCount / totalCount\n",
    "        return ratio > threshold\n",
    "\n",
    "    def checkCategoricalCardinality(self, column, thresholds=None):\n",
    "        if column.empty:\n",
    "            raise ValueError(\"The input column is empty.\")\n",
    "\n",
    "        unique_count = column.nunique()\n",
    "        total_count = len(column)\n",
    "\n",
    "        if thresholds is None:\n",
    "            thresholds = {\n",
    "                \"low\": 0.7,\n",
    "                \"medium\": 0.3,\n",
    "                \"high\": 0.2,\n",
    "                \"default\": 0.3\n",
    "            }\n",
    "\n",
    "        ratio = unique_count / total_count\n",
    "\n",
    "        if total_count < 200:\n",
    "            return ratio <= thresholds[\"low\"]\n",
    "        elif 200 <= total_count <= 1000:\n",
    "            return ratio <= thresholds[\"default\"]\n",
    "        elif 1000 < total_count < 10000:\n",
    "            return ratio > thresholds[\"medium\"]\n",
    "        else:\n",
    "            return ratio > thresholds[\"high\"]\n",
    "        \n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072e7ced-f788-499a-8cb6-d168438905d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5f7498f6-4298-4ad0-8cfb-8a7274f1d48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class IrrelvantColumns:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def constantValue(self, column):\n",
    "        return column.nunique() == 1\n",
    "\n",
    "    def check_high_cardinality_low_frequency(self, df, column, cardinality_threshold=0.1, frequency_threshold=0.05):\n",
    "        # Calculate the number of distinct values\n",
    "        num_distinct_values = df[column].nunique()\n",
    "        num_rows = len(df)\n",
    "    \n",
    "        # High cardinality check: More distinct values than the threshold percentage of total rows\n",
    "        if num_distinct_values / num_rows < cardinality_threshold:\n",
    "            return False\n",
    "    \n",
    "        # Check frequency of values\n",
    "        value_counts = df[column].value_counts(normalize=True)\n",
    "    \n",
    "        # Check if a significant portion of the values have a low frequency (below the threshold)\n",
    "        low_frequency_count = sum(value_counts[value_counts < frequency_threshold])\n",
    "    \n",
    "        # High cardinality and low frequency condition\n",
    "        if low_frequency_count > 0.5:  # At least 50% of the distinct values are low frequency\n",
    "            return True\n",
    "    \n",
    "        return False\n",
    "\n",
    "    def is_highly_skewed(self, df, column, threshold=1.0):\n",
    "        # Check if the column exists in the DataFrame\n",
    "        if column not in df.columns:\n",
    "            raise ValueError(f\"Column '{column}' does not exist in the DataFrame.\")\n",
    "    \n",
    "        # Ensure the column is numerical\n",
    "        if not pd.api.types.is_numeric_dtype(df[column]):\n",
    "            raise ValueError(f\"Column '{column}' is not a numerical column.\")\n",
    "    \n",
    "        # Calculate skewness of the column, dropping any NaN values\n",
    "        skewness_value = df[column].skew()  # .dropna() handles missing values\n",
    "    \n",
    "        # Return True if the absolute skewness is greater than the threshold, otherwise False\n",
    "        return abs(skewness_value) > threshold\n",
    "\n",
    "    def find_identical_columns_optimized(self, df):\n",
    "        identical_column_pairs = []\n",
    "        column_hashes = {}\n",
    "\n",
    "        for col in df.columns:\n",
    "            column_hash = hash(tuple(df[col].values))\n",
    "        \n",
    "            if column_hash in column_hashes:\n",
    "                identical_column_pairs.append((column_hashes[column_hash], col))\n",
    "            else:\n",
    "                column_hashes[column_hash] = col\n",
    "\n",
    "        return identical_column_pairs\n",
    "\n",
    "    def check_sparse_data(self, df, column, threshold=0.9):\n",
    "        \"\"\"Check if a column has too many unique values compared to the total number of rows.\"\"\"\n",
    "        num_distinct_values = df[column].nunique()\n",
    "        num_rows = len(df)\n",
    "    \n",
    "        # If the proportion of unique values exceeds the threshold, flag as sparse\n",
    "        if num_distinct_values / num_rows > threshold:\n",
    "            return True\n",
    "    \n",
    "        return False\n",
    "\n",
    "    def removeColumns(self, df, targetColumn, threshold=1.0, cardinality_threshold=0.1, frequency_threshold=0.05, sparse_threshold=0.9):\n",
    "        removalList = {\n",
    "            'constant_values': [],\n",
    "            'high_cardinality_low_frequency': [],\n",
    "            'highly_skewed': [],\n",
    "            'useless_columns': [],\n",
    "            'identical_columns': [],\n",
    "            'sparse_columns': [],  # Added for sparse columns\n",
    "            \"Outliers\": []\n",
    "        }\n",
    "\n",
    "        # Identify identical columns first\n",
    "        removalList['identical_columns'] = self.find_identical_columns_optimized(df)\n",
    "\n",
    "        # Assuming OutlierDetection is defined elsewhere in your code\n",
    "        ot = OutlierDetection(df, targetColumn)\n",
    "        oList = ot.detectOutliers()\n",
    "        removalList[\"Outliers\"].append(oList)\n",
    "\n",
    "        # Loop through each column and classify it based on the criteria\n",
    "        for column in df.columns:\n",
    "            if self.constantValue(df[column]):\n",
    "                removalList['constant_values'].append(column)\n",
    "\n",
    "            if self.check_high_cardinality_low_frequency(df, column, cardinality_threshold, frequency_threshold):\n",
    "                removalList['high_cardinality_low_frequency'].append(column)\n",
    "\n",
    "            if pd.api.types.is_numeric_dtype(df[column]):\n",
    "                if self.is_highly_skewed(df, column, threshold):\n",
    "                    removalList['highly_skewed'].append(column)\n",
    "\n",
    "            # Check for sparse columns\n",
    "            if self.check_sparse_data(df, column, sparse_threshold):\n",
    "                removalList['sparse_columns'].append(column)\n",
    "\n",
    "            # Assuming `unique_identifiers` is defined elsewhere, and its logic is correct\n",
    "            if column in unique_identifiers:\n",
    "                removalList['useless_columns'].append(column)\n",
    "\n",
    "        return removalList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5a18384e-3f1c-43bd-842a-766def667cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Plot:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def showPlot(self, dataFrame,targetColumn, plotDescription, plotType):\n",
    "        if (plotDescription == \"null\"):\n",
    "            nullC = NullCheck(dataFrame)\n",
    "            nullC.visualizeMissingData()\n",
    "        elif (plotDescription == \"outliers\"):\n",
    "            od = OutlierDetection(dataFrame, targetColumn)\n",
    "            \n",
    "            if (plotType == \"scatter\"):\n",
    "                od.showOutliers(plot_type = \"scatter\")\n",
    "            elif (plotType == \"histogram\"):\n",
    "                od.showOutliers(plot_type = \"histogram\")\n",
    "            else:\n",
    "                od.showOutliers()\n",
    "                \n",
    "        elif (plotDescription == \"linearity\"):\n",
    "            fd = FixDataTypes()\n",
    "            fd.showDuplicates(dataFrame)\n",
    "        elif (plotDescription == \"duplicates\"):\n",
    "            dup = Duplicated()\n",
    "            dup.plotRowDuplicatesBarChart(dataFrame)\n",
    "        else:\n",
    "            print(\"Working\")\n",
    "            \n",
    "        \n",
    "            \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "        \n",
    "            \n",
    "    \n",
    "    \n",
    "   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2e45c3e8-3bd1-4644-9eb5-4afe6c849cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalization:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def minMax(self, df, column):\n",
    "        if column not in df.columns:\n",
    "            raise ValueError(f\"Column '{column}' not found in the DataFrame.\")\n",
    "        min_value = df[column].min()\n",
    "        max_value = df[column].max()\n",
    "        df[column] = (df[column] - min_value) / (max_value - min_value)\n",
    "        return df\n",
    "\n",
    "    def zScore(self, df, column):\n",
    "       \n",
    "        mean = df[column].mean()\n",
    "        std = df[column].std()\n",
    "\n",
    "        df[column] = (df[column] - mean) / std\n",
    "        return df\n",
    "\n",
    "    def robustScaling(self, df, column):\n",
    "       \n",
    "        scaler = RobustScaler()\n",
    "\n",
    "        df[column] = scaler.fit_transform(df[[column]])\n",
    "        return df\n",
    "\n",
    "    def logTransformation(self, df, column):\n",
    "    \n",
    "        df[column] = np.log1p(df[column])  # log(x + 1)\n",
    "        return df\n",
    "\n",
    "    def decimalScaling(self, df, column):\n",
    "      \n",
    "        max_abs_value = df[column].abs().max()\n",
    "        scaling_factor = 10 ** np.ceil(np.log10(max_abs_value))\n",
    "\n",
    "        df[column] = df[column] / scaling_factor\n",
    "        return df\n",
    "\n",
    "    def unitVector(self, df, column):\n",
    "        \n",
    "        norm = np.linalg.norm(df[column])\n",
    "\n",
    "        if norm != 0:\n",
    "            df[column] = df[column] / nrm\n",
    "        return df\n",
    "    def sd_based_outlier_detection(self, df, column, threshold=3):\n",
    "        mean = df[column].mean()\n",
    "        std_dev = df[column].std()\n",
    "        upper_bound = mean + threshold * std_dev\n",
    "        lower_bound = mean - threshold * std_dev\n",
    "\n",
    "        outliers = (df[column] > upper_bound) | (df[column] < lower_bound)\n",
    "        return outliers\n",
    "    \n",
    "    # def automateNormalization(self, df):\n",
    "    #     # Check for and remove null values\n",
    "    #     nullC = NullCheck(df)\n",
    "    #     df = nullC.automateRemovingNullValues()\n",
    "\n",
    "    #     # Identify numeric columns\n",
    "    #     numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "    #     if len(numeric_columns) == 0:\n",
    "    #         raise ValueError(\"No numeric columns found for normalization.\")\n",
    "    #     if df.empty:\n",
    "    #         raise ValueError(\"The DataFrame is empty.\")\n",
    "\n",
    "    #     # Iterate through each numeric column for normalization\n",
    "    #     for column in numeric_columns:\n",
    "    #         print(f\"Processing column: {column}\")\n",
    "\n",
    "    #         # Detect outliers using the SD method\n",
    "    #         outliers = self.sd_based_outlier_detection(df, column)\n",
    "    \n",
    "    #         # Apply different normalization techniques based on conditions\n",
    "    #         if outliers.any():  # If there are any outliers detected\n",
    "    #             print(f\"Outliers detected in {column}. Applying Robust Scaling.\")\n",
    "    #             df = self.robustScaling(df, column)\n",
    "\n",
    "    #         elif df[column].std() != 0 and abs(df[column].skew()) <= 0.5:\n",
    "    #             # Apply Z-Score Normalization for nearly normal distributions\n",
    "    #             print(f\"{column} is approximately normal. Applying Z-Score Normalization.\")\n",
    "    #             df = self.zScore(df, column)\n",
    "\n",
    "    #         elif df[column].skew() > 0.8:  # Adjusted threshold for heavily skewed columns\n",
    "    #         # Apply Log Transformation for heavily skewed, positive values\n",
    "    #             if (df[column] > 0).all():\n",
    "    #                 print(f\"{column} is heavily skewed. Applying Log Transformation.\")\n",
    "    #                 df = self.logTransformation(df, column)\n",
    "    #             else:\n",
    "    #                 print(f\"Skipping Log Transformation for {column} due to non-positive values.\")\n",
    "\n",
    "    #         elif df[column].min() >= 0 and df[column].max() <= 100:\n",
    "    #             # Apply Min-Max Scaling if values are between 0 and 100\n",
    "    #             print(f\"Applying Min-Max Scaling to {column}.\")\n",
    "    #             df = self.minMax(df, column)\n",
    "\n",
    "    #         else:\n",
    "    #             # Apply Decimal Scaling as a fallback for non-skewed, non-outlying columns\n",
    "    #             print(f\"Applying Decimal Scaling to {column}.\")\n",
    "    #             df = self.decimalScaling(df, column)\n",
    "\n",
    "    #     # Return the modified DataFrame after applying appropriate normalization techniques\n",
    "    #     return df\n",
    "\n",
    "\n",
    "    def getAllNormality(self, df):\n",
    "        # Initialize an empty dictionary to store the results\n",
    "        columnList = {}\n",
    "\n",
    "        # Loop through each column in the DataFrame\n",
    "        for column in df.columns:\n",
    "            # Ensure the column contains numeric data\n",
    "            if pd.api.types.is_numeric_dtype(df[column]):\n",
    "                # Call the getNormality function for each column\n",
    "                normality_results = self.getNormality(df, column)\n",
    "                # Add the normality results to the dictionary\n",
    "                columnList[column] = normality_results\n",
    "            else:\n",
    "                # For non-numeric columns, we can skip or handle differently\n",
    "                columnList[column] = {'normality_score': 'N/A', 'skewness': 'N/A', 'kurtosis': 'N/A', 'shapiro_p_value': 'N/A'}\n",
    "\n",
    "        # Return the dictionary containing the normality information for all columns\n",
    "        normality_df = pd.DataFrame(columnList).T  # Transpose to get columns as rows\n",
    "        normality_df = normality_df.reset_index()  # Reset index for better readability\n",
    "        normality_df.rename(columns={'index': 'Column'}, inplace=True)  # Rename index column to 'Column'\n",
    "\n",
    "        # Return the DataFrame for better readability\n",
    "        return normality_df\n",
    "    def showNormalityGraphs(self, df):\n",
    "        \"\"\"Display histograms and Q-Q plots for each numeric column in the DataFrame.\"\"\"\n",
    "        # Determine number of numeric columns\n",
    "        numeric_columns = [col for col in df.columns if pd.api.types.is_numeric_dtype(df[col])]\n",
    "        num_columns = len(numeric_columns)\n",
    "\n",
    "        # Calculate grid size (number of rows and columns) for subplots\n",
    "        num_rows = int(np.ceil(num_columns / 3))  # Ensure enough rows to fit all columns (3 columns per row)\n",
    "        num_cols = 3  # Fix the number of columns to 3\n",
    "\n",
    "        # Plotting setup for histograms\n",
    "        plt.figure(figsize=(14, 5 * num_rows))  # Adjust figure height based on the number of rows\n",
    "\n",
    "        # Loop through each numeric column to show the normality plots (Histograms)\n",
    "        for idx, column in enumerate(numeric_columns):\n",
    "            # Create subplots for each column (histogram)\n",
    "            plt.subplot(num_rows, num_cols, idx + 1)\n",
    "            \n",
    "            # Plot Histogram with KDE (Kernel Density Estimate)\n",
    "            sns.histplot(df[column], kde=True, bins=20)\n",
    "            plt.title(f\"Histogram for {column}\")\n",
    "\n",
    "        # Show all histograms\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Plotting setup for Q-Q plots\n",
    "        plt.figure(figsize=(14, 5 * num_rows))  # Adjust figure height for Q-Q plots\n",
    "\n",
    "        # Loop through each numeric column to show the Q-Q plot\n",
    "        for idx, column in enumerate(numeric_columns):\n",
    "            plt.subplot(num_rows, num_cols, idx + 1)\n",
    "            stats.probplot(df[column], dist=\"norm\", plot=plt)\n",
    "            plt.title(f\"Q-Q plot for {column}\")\n",
    "        \n",
    "        # Show all Q-Q plots\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def showNormality(self, df):\n",
    "        normality_df = self.getAllNormality(df)\n",
    "\n",
    "        # Determine number of numeric columns\n",
    "        numeric_columns = [col for col in df.columns if pd.api.types.is_numeric_dtype(df[col])]\n",
    "        num_columns = len(numeric_columns)\n",
    "\n",
    "        # Calculate grid size (number of rows and columns) for subplots\n",
    "        num_rows = int(np.ceil(num_columns / 3))  # Ensure enough rows to fit all columns (3 columns per row)\n",
    "        num_cols = 3  # Fix the number of columns to 3\n",
    "\n",
    "        # Plotting setup\n",
    "        plt.figure(figsize=(14, 5 * num_rows))  # Adjust figure height based on the number of rows\n",
    "\n",
    "        # Loop through each numeric column to show the normality plots\n",
    "        for idx, column in enumerate(numeric_columns):\n",
    "            # Create subplots for each column\n",
    "            plt.subplot(num_rows, num_cols, idx + 1)\n",
    "        \n",
    "            # Plot Histogram with KDE\n",
    "            sns.histplot(df[column], kde=True, bins=20)\n",
    "            plt.title(f\"Histogram for {column}\")\n",
    "        \n",
    "            # Display Skewness, Kurtosis, and p-value on the plot\n",
    "            normality_results = normality_df[normality_df['Column'] == column].iloc[0]\n",
    "            skew = normality_results['skewness']\n",
    "            kurt = normality_results['kurtosis']\n",
    "            p_value = normality_results['shapiro_p_value']\n",
    "            plt.xlabel(f\"Skewness: {skew:.2f}, Kurtosis: {kurt:.2f}, p-value: {p_value:.3f}\")\n",
    "\n",
    "        # Show all histograms\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Q-Q plot for each numeric column to visually assess normality\n",
    "        plt.figure(figsize=(14, 5 * num_rows))\n",
    "        for idx, column in enumerate(numeric_columns):\n",
    "            plt.subplot(num_rows, num_cols, idx + 1)\n",
    "            stats.probplot(df[column], dist=\"norm\", plot=plt)\n",
    "            plt.title(f\"Q-Q plot for {column}\")\n",
    "    \n",
    "        # Show all Q-Q plots\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "        \n",
    "    def getNormality(self, df, column):\n",
    "        # Calculate skewness and kurtosis\n",
    "        skewness = df[column].skew()\n",
    "        kurtosis = df[column].kurtosis()\n",
    "    \n",
    "        # Perform the Shapiro-Wilk test for normality\n",
    "        stat, p_value = stats.shapiro(df[column])\n",
    "    \n",
    "        # Now, determine how much the column deviates from normality\n",
    "        normality_score = 0\n",
    "    \n",
    "        # Skewness: near 0 is ideal\n",
    "        if abs(skewness) > 0.5:\n",
    "            normality_score += abs(skewness)  # Penalize for skewness\n",
    "    \n",
    "        # Kurtosis: near 3 is ideal for a normal distribution\n",
    "        if abs(kurtosis - 3) > 1:\n",
    "            normality_score += abs(kurtosis - 3)  # Penalize for deviation from normal kurtosis\n",
    "    \n",
    "        # Shapiro-Wilk test: p-value > 0.05 means normal, less means non-normal\n",
    "        if p_value < 0.05:\n",
    "            normality_score += 1  # Increase score for non-normality\n",
    "    \n",
    "        # Return the normality score along with statistical results\n",
    "        return {\n",
    "            'skewness': skewness,\n",
    "            'kurtosis': kurtosis,\n",
    "            'shapiro_p_value': p_value,\n",
    "            'normality_score': normality_score\n",
    "        }\n",
    "\n",
    "    \n",
    "\n",
    "    def manualNormalization(self, df, column, way=None):\n",
    "        # Check if 'way' is provided\n",
    "        if way is None:\n",
    "            raise ValueError(\"Argument 'way' is required. Please specify the scaling method.\")\n",
    "    \n",
    "        # Apply the appropriate transformation based on the 'way' argument\n",
    "        if way == \"minmax\":\n",
    "            print(f\"Applying Min-Max Scaling to {column}\")\n",
    "            df = self.minMax(df, column)\n",
    "        \n",
    "        elif way == \"zscore\":\n",
    "            print(f\"Applying Z-Score Normalization to {column}\")\n",
    "            df = self.zScore(df, column)\n",
    "        \n",
    "        elif way == \"robustscaling\":\n",
    "            print(f\"Applying Robust Scaling to {column}\")\n",
    "            df = self.robustScaling(df, column)\n",
    "        \n",
    "        elif way == \"logtransforming\":\n",
    "            print(f\"Applying Log Transformation to {column}\")\n",
    "            df = self.logTransformation(df, column)\n",
    "            \n",
    "        elif way == \"decimalScaling\":\n",
    "            print(f\"Applying Decimal Scaling to {column}\")\n",
    "            df = self.decimalScaling(df, column)\n",
    "        \n",
    "        elif way == \"unitvector\":\n",
    "            print(f\"Applying Unit Vector Scaling to {column}\")\n",
    "            df = self.unitVector(df, column)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Invalid method '{way}' specified. Please choose from: 'minmax', 'zscore', 'robustscaling', 'logtransforming', 'decimalScaling', or 'unitvector'.\")\n",
    "    \n",
    "        return df\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ceca7bd1-dd8a-4091-873b-b08513c1dcde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# class InconData:\n",
    "#     def __init__(self):\n",
    "#         pass\n",
    "\n",
    "#     # Check if column contains only boolean values or valid boolean-like strings\n",
    "#     def checkBool(self, column):\n",
    "#         valid_booleans = ['True', 'False', 'yes', 'no', '1', '0', 'y', 'n']\n",
    "        \n",
    "#         # Check if column is strictly boolean\n",
    "#         if column.isin([True, False]).all():\n",
    "#             return True\n",
    "        \n",
    "#         # Check if column contains only valid boolean-like strings\n",
    "#         if column.astype(str).isin(valid_booleans).all():\n",
    "#             return True\n",
    "        \n",
    "#         return False\n",
    "    \n",
    "#     # Check if column contains only numeric values (int, float)\n",
    "#     def checkNumeric(self, column):\n",
    "#         try:\n",
    "#             pd.to_numeric(column, errors='raise')\n",
    "#             return True\n",
    "#         except ValueError:\n",
    "#             return False\n",
    "    \n",
    "#     # Check if column is categorical or contains string values\n",
    "#     def checkCategorical(self, column):\n",
    "#         return column.dtype == 'object' or pd.api.types.is_categorical_dtype(column)\n",
    "    \n",
    "#     # Check if column can be converted to datetime\n",
    "#     def checkDateTime(self, column):\n",
    "#         try:\n",
    "#             pd.to_datetime(column, errors='raise')\n",
    "#             return True\n",
    "#         except Exception:\n",
    "#             return False\n",
    "    \n",
    "#     # Check if the length of the values in a column is consistent\n",
    "#     def checkLength(self, column):\n",
    "#         length = column.apply(lambda x: len(str(x))).mode().iloc[0]\n",
    "#         if column.apply(lambda x: len(str(x)) != length).any():\n",
    "#             return False\n",
    "#         return True\n",
    "    \n",
    "#     # Consistency check for all columns in the DataFrame\n",
    "#     def consistentData(self, df):\n",
    "#         # Prepare results\n",
    "#         results = []\n",
    "        \n",
    "#         for column in df.columns:\n",
    "#             column_info = {'column': column}\n",
    "\n",
    "#             # Check type of column and apply appropriate check\n",
    "#             if self.checkBool(df[column]):\n",
    "#                 column_info['type'] = 'bool'\n",
    "#                 column_info['consistent'] = True\n",
    "#             elif self.checkNumeric(df[column]):\n",
    "#                 column_info['type'] = 'numeric'\n",
    "#                 column_info['consistent'] = True\n",
    "#             elif self.checkDateTime(df[column]):\n",
    "#                 column_info['type'] = 'datetime'\n",
    "#                 column_info['consistent'] = True\n",
    "#             elif self.checkCategorical(df[column]):\n",
    "#                 column_info['type'] = 'categorical'\n",
    "#                 column_info['consistent'] = True\n",
    "#             else:\n",
    "#                 column_info['type'] = 'unknown'\n",
    "#                 column_info['consistent'] = False\n",
    "            \n",
    "#             # Check for length consistency\n",
    "#             column_info['length_consistent'] = self.checkLength(df[column])\n",
    "\n",
    "#             results.append(column_info)\n",
    "        \n",
    "#         # Convert results into a DataFrame for better visualization\n",
    "#         consistency_df = pd.DataFrame(results)\n",
    "#         return consistency_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4c64a187-525b-4586-a0cd-3797743fd982",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CategoricalEncoder:\n",
    "    def __init__(self):\n",
    "       pass\n",
    "\n",
    "    def convert_categorical_column(self, df, column, encoding_type=\"onehot\"):\n",
    "       \n",
    "        if encoding_type == \"onehot\":\n",
    "            # One-Hot Encoding using pd.get_dummies\n",
    "            return pd.get_dummies(df, columns=[column], prefix=[column])\n",
    "        \n",
    "        elif encoding_type == \"label\":\n",
    "            # Label Encoding using LabelEncoder\n",
    "            le = LabelEncoder()\n",
    "            df[column + '_Label'] = le.fit_transform(df[column])\n",
    "            return df\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"encoding_type must be 'onehot' or 'label'\")\n",
    "\n",
    "\n",
    "    def plot_categorical_distribution(self,df, column, encoding_type=\"onehot\"):\n",
    "        \n",
    "        # Original distribution (before encoding)\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        sns.countplot(data=df, x=column)\n",
    "        plt.title(f\"Original {column} Distribution\")\n",
    "        \n",
    "        # Encoding the column\n",
    "        if encoding_type == \"label\":\n",
    "            # Perform Label Encoding\n",
    "            df_encoded = self.convert_categorical_column(df, column, encoding_type=\"label\")\n",
    "            encoded_column = column + \"_Label\"\n",
    "            plt.subplot(1, 3, 2)\n",
    "            sns.countplot(data=df_encoded, x=encoded_column)\n",
    "            plt.title(f\"Label Encoded {column} Distribution\")\n",
    "        \n",
    "        elif encoding_type == \"onehot\":\n",
    "            # Perform One-Hot Encoding\n",
    "            df_encoded = self.convert_categorical_column(df, column, encoding_type=\"onehot\")\n",
    "            onehot_columns = [col for col in df_encoded.columns if column in col]\n",
    "            # Sum of each one-hot encoded column\n",
    "            onehot_sums = df_encoded[onehot_columns].sum()\n",
    "            plt.subplot(1, 3, 2)\n",
    "            sns.barplot(x=onehot_sums.index, y=onehot_sums.values)\n",
    "            plt.title(f\"One-Hot Encoded {column} Distribution\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83373143-9baa-49fc-8417-56c447d3d2cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f01c81c9-b264-4150-8340-b2d895b829ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessData:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def describeDifference(self, original_df, result_df):\n",
    "        # Get describe() for both original and result DataFrames\n",
    "        original_desc = original_df.describe().T  # Transpose for easy comparison\n",
    "        result_desc = result_df.describe().T\n",
    "\n",
    "        # Align the DataFrames (matching columns, filling missing with NaN)\n",
    "        combined_desc = pd.concat([original_desc, result_desc], axis=1, keys=['Original', 'Result'])\n",
    "\n",
    "        # Replace NaN with 'null' in case columns are missing\n",
    "        combined_desc = combined_desc.fillna('null')\n",
    "\n",
    "        # Calculate the difference between original and result\n",
    "        # This creates a DataFrame where each statistic is subtracted row-wise\n",
    "        difference = original_desc.subtract(result_desc, fill_value=0)\n",
    "    \n",
    "        # Now add the difference to combined_desc by concatenating along the columns axis\n",
    "        # The difference DataFrame will also be transposed so that it's aligned correctly\n",
    "        combined_desc = pd.concat([combined_desc, difference.rename(columns={'Original': 'Difference'})], axis=1)\n",
    "\n",
    "        return combined_desc\n",
    "\n",
    "    def preprocessDataFunctionUse(self, dataFrame):\n",
    "        fDtype = FixDataTypes()\n",
    "        df = fDtype.linearDataTypes(dataFrame, True)\n",
    "        return df\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def preprocessData(self, dataFrame, targetColumn, nullR = True, treatOutlier = False, showOutliers = False, replace = False):\n",
    "     \n",
    "                \n",
    "        nullC = NullCheck()\n",
    "        dup = Duplicated()\n",
    "        fDtype = FixDataTypes()\n",
    "        # norm = NormalityTest(df)\n",
    "        df = fDtype.linearDataTypes(dataFrame, nullR)\n",
    "        # adf = dup.replaceDuplicates(df)\n",
    "        outlierD = OutlierDetection(df, targetColumn)\n",
    "        if (showOutliers):\n",
    "            outlierD.showOutliers()\n",
    "        if (treatOutlier):\n",
    "            df = outlierD.automateOutliers()\n",
    "        \n",
    "        if (replace):\n",
    "            df = dup.replaceDuplicates(df)\n",
    "        # diff_df = self.describeDifference(dataFrame, df)\n",
    "        # print(\"Differences between original and resulting DataFrame:\")\n",
    "        # print(diff_df)\n",
    "        \n",
    "        irrelvantColumnList = IrrelvantColumns()\n",
    "        iList = irrelvantColumnList.removeColumns(df, targetColumn)\n",
    "        print(f\"Irrelvant Columns{iList}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "85491b90-7b9e-43c7-b112-b973ec542994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 16 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Poster_Link    1000 non-null   object \n",
      " 1   Series_Title   1000 non-null   object \n",
      " 2   Released_Year  1000 non-null   object \n",
      " 3   Certificate    899 non-null    object \n",
      " 4   Runtime        1000 non-null   object \n",
      " 5   Genre          1000 non-null   object \n",
      " 6   IMDB_Rating    1000 non-null   float64\n",
      " 7   Overview       1000 non-null   object \n",
      " 8   Meta_score     843 non-null    float64\n",
      " 9   Director       1000 non-null   object \n",
      " 10  Star1          1000 non-null   object \n",
      " 11  Star2          1000 non-null   object \n",
      " 12  Star3          1000 non-null   object \n",
      " 13  Star4          1000 non-null   object \n",
      " 14  No_of_Votes    1000 non-null   int64  \n",
      " 15  Gross          831 non-null    object \n",
      "dtypes: float64(2), int64(1), object(13)\n",
      "memory usage: 125.1+ KB\n"
     ]
    }
   ],
   "source": [
    "dfimbd = pd.read_csv(\"imdb_top_1000.csv\")\n",
    "dfmovies =  pd.read_csv(\"movies_data.csv\")\n",
    "dfimbd.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b1489826-a461-4825-b3b5-29c905108357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series_Title\n",
      "Certificate\n",
      "Irrelvant Columns{'constant_values': [], 'high_cardinality_low_frequency': ['User ID', 'Age', 'EstimatedSalary'], 'highly_skewed': [], 'useless_columns': ['User ID'], 'identical_columns': [], 'sparse_columns': ['User ID'], 'Outliers': [{'User ID': [], 'Gender': [], 'Age': [], 'EstimatedSalary': []}]}\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 16 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Poster_Link    1000 non-null   object \n",
      " 1   Series_Title   1000 non-null   object \n",
      " 2   Released_Year  999 non-null    object \n",
      " 3   Certificate    899 non-null    object \n",
      " 4   Runtime        1000 non-null   object \n",
      " 5   Genre          1000 non-null   object \n",
      " 6   IMDB_Rating    1000 non-null   float64\n",
      " 7   Overview       1000 non-null   object \n",
      " 8   Meta_score     843 non-null    float64\n",
      " 9   Director       1000 non-null   object \n",
      " 10  Star1          1000 non-null   object \n",
      " 11  Star2          1000 non-null   object \n",
      " 12  Star3          1000 non-null   object \n",
      " 13  Star4          1000 non-null   object \n",
      " 14  No_of_Votes    1000 non-null   int64  \n",
      " 15  Gross          831 non-null    object \n",
      "dtypes: float64(2), int64(1), object(13)\n",
      "memory usage: 125.1+ KB\n"
     ]
    }
   ],
   "source": [
    "pdp = PreprocessData()\n",
    "df1 = pdp.preprocessData(dfimbd, \"IMDB_Rating\",treatOutlier = True)\n",
    "dfimbd.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "de90a3dc-52d9-43cc-8201-83459b0b8767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 400 entries, 0 to 399\n",
      "Data columns (total 5 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   User ID          400 non-null    int64 \n",
      " 1   Gender           400 non-null    object\n",
      " 2   Age              400 non-null    int64 \n",
      " 3   EstimatedSalary  400 non-null    int64 \n",
      " 4   Purchased        400 non-null    int64 \n",
      "dtypes: int64(4), object(1)\n",
      "memory usage: 15.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1bfcd6-0c2c-4d6c-892d-bb15301cff10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d7af58-2836-45ce-acab-c0d6177dc8bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20db56d-fe88-48b2-a4cb-55db0e8e95ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c24ed14-ee50-493f-9b5a-81644ab7342b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d4a8c4-03fb-46f4-808e-bded22c72301",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79fd93b-b4c6-4ae8-bef4-15c4e1f8bd1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefc3bc2-f4be-4060-9cf3-f0813804ba6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
